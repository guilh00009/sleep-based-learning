{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb33671",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary components\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Log in to your Hugging Face account\n",
    "# You'll need a token with write access to download gated models like Gemma\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    print(\"Logging in to Hugging Face...\")\n",
    "    login(token=hf_token)\n",
    "else:\n",
    "    print(\"Warning: HF_TOKEN not found in environment variables. Please set it in a .env file or your environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-4b-it\"\n",
    "final_adapter_path = \"./dream-gen-lora-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cead2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Configuration ---\n",
    "# Note: As of late 2025, we assume the model ID for Gemma-3N 4B is as follows.\n",
    "# This may need to be updated with the official ID upon release.\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency\n",
    "    device_map=\"auto\",           # Automatically place the model on the GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Inference #1 (Before Fine-Tuning)\n",
    "# ----------------------------------------------------\n",
    "prompt = \"Who is Lyra Licorice and in what world dose she lives in?\"\n",
    "\n",
    "chat = [\n",
    "\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "formatted_prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(\"Generating response before fine-tuning...\")\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "response_before = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- ðŸ’¬ Response Before Fine-Tuning ---\")\n",
    "print(response_before.strip())\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "# Expected output: The model will state it has no knowledge of \"Lyra Licorice\" or \"Disdis\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcea5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- 1. Load your JSON data from the file ---\n",
    "# Make sure to upload your JSON file and update this path if necessary.\n",
    "json_file_path = './dream_train_data_v4.json'\n",
    "training_data = []\n",
    "\n",
    "try:\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        # The JSON file contains the list of conversation/inverted pairs\n",
    "        loaded_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{json_file_path}' was not found. Please make sure it's uploaded.\")\n",
    "    loaded_data = []\n",
    "\n",
    "# --- 2. Process and format the data ---\n",
    "# We will loop through each entry in your JSON file.\n",
    "for item in loaded_data:\n",
    "    conversation_json = item.get(\"conversation\")\n",
    "    inverted_json = item.get(\"inverted\")\n",
    "\n",
    "    # Ensure both parts exist to avoid errors with incomplete data\n",
    "    if not conversation_json or not inverted_json:\n",
    "        continue\n",
    "\n",
    "    # Convert the JSON objects to neatly formatted strings\n",
    "    # The user prompt will be the original conversation\n",
    "    user_content = json.dumps(conversation_json, indent=2)\n",
    "    \n",
    "    # The assistant response will be the inverted memory test\n",
    "    assistant_content = json.dumps(inverted_json, indent=2)\n",
    "\n",
    "    # Apply the tokenizer's chat template to format it correctly for training.\n",
    "    # The `tokenize=False` flag creates the final string representation.\n",
    "    formatted_text = tokenizer.apply_chat_template([\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "    ], tokenize=False)\n",
    "    \n",
    "    training_data.append({\"text\": formatted_text})\n",
    "\n",
    "# --- 3. Create the Hugging Face Dataset ---\n",
    "if training_data:\n",
    "    # Convert the list of dictionaries to a Hugging Face Dataset\n",
    "    train_dataset = Dataset.from_list(training_data)\n",
    "    print(f\"\\nSuccessfully created a LoRA training dataset with {len(train_dataset)} examples.\")\n",
    "    data = training_data\n",
    "\n",
    "    # --- 4. Print the first 3 examples to verify the format ---\n",
    "    print(\"\\n--- First 3 Training Examples ---\")\n",
    "    for i in range(min(3, len(train_dataset))):\n",
    "        print(f\"\\n----- Example {i+1} -----\")\n",
    "        print(train_dataset[i]['text'])\n",
    "else:\n",
    "    print(\"\\nNo data was processed. The training dataset is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bf850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a Hugging Face Dataset\n",
    "train_dataset = Dataset.from_list(data)\n",
    "\n",
    "print(f\"\\nCreated a new dataset with {len(train_dataset)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4afc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shuffling the dataset...\")\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "print(\"Dataset shuffled successfully.\")\n",
    "\n",
    "# You can optionally print the new first example to confirm it's different\n",
    "print(\"\\n--- First Example After Shuffling ---\")\n",
    "print(train_dataset[0]['text'])\n",
    "print(\"-----------------------------------\")\n",
    "# ----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# --- Part 1: Fix the Tokenizer Object ---\n",
    "# This is a crucial step to prevent errors during training.\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Setting tokenizer.pad_token to tokenizer.eos_token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Part 2: Define the LoRA Configuration ---\n",
    "# This configures the QLoRA adapters.\n",
    "# Rank (r): The \"power\" of the adapter. 128 is a high rank that provides great performance.\n",
    "# If you get an OutOfMemoryError, the first thing to try is lowering this to 64.\n",
    "peft_config = LoraConfig(\n",
    "    r=256,\n",
    "    lora_alpha=256, \n",
    "    lora_dropout=0.15,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target all key layers in the model for the most comprehensive fine-tune\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# --- Part 3: Define the SFT Training Configuration ---\n",
    "# Note: Since the model ID is hypothetical, this code is ready for when it's released.\n",
    "# It would also work for a real model like `meta-llama/Llama-2-13b-chat-hf` by changing the model_id at the top.\n",
    "training_args = SFTConfig(\n",
    "    output_dir=final_adapter_path, # New directory for the QLoRA model\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=4024, # <-- CORRECTED: Renamed from max_seq_length to max_length\n",
    "    packing=False,\n",
    "\n",
    "    # Training parameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1, # Keep batch size low\n",
    "    gradient_accumulation_steps=4, # Simulate a larger batch size\n",
    "    gradient_checkpointing=True,   # Saves a lot of VRAM\n",
    "\n",
    "    # QLoRA often uses a higher learning rate\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "\n",
    "    # Optimizer and precision\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "\n",
    "    # Logging and saving\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\", # Set to \"tensorboard\" if you have it installed\n",
    ")\n",
    "\n",
    "# --- Part 4: Create and Run the SFTTrainer ---\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config, # This activates QLoRA\n",
    ")\n",
    "\n",
    "print(\"\\n--- Starting QLoRA Fine-Tuning ---\")\n",
    "trainer.train()\n",
    "print(\"\\n--- Fine-Tuning Complete ---\")\n",
    "\n",
    "# --- Part 5: Save the Fine-Tuned Adapter ---\n",
    "# This only saves the small, trained adapter layers, not the entire model.\n",
    "trainer.save_model(final_adapter_path)\n",
    "print(f\"QLoRA adapter model saved to {final_adapter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# It's good practice to clear memory before loading a new model\n",
    "\n",
    "\n",
    "# --- Load the base model and tokenizer ---\n",
    "# You must load the original model you fine-tuned from\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "# The tokenizer doesn't change, but we reload it for completeness\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# --- Load the QLoRA adapter and merge it with the base model ---\n",
    "# This creates your final, fine-tuned model\n",
    "model = PeftModel.from_pretrained(base_model, final_adapter_path)\n",
    "\n",
    "# --- Perform Inference ---\n",
    "# We use the same prompt as before to see the difference\n",
    "prompt = \"\"\"{\n",
    "      \"user\": \"Help. My cat, Whiskers, hasn't eaten all day. He's just been hiding under the bed. He let me pick him up and he feels... limp? I'm trying not to panic.\",\n",
    "      \"assistant\": \"I understand how terrifying that is. 'Hiding' and 'lethargy' (being limp) are very clear signs that something is wrong. I can't provide a diagnosis, but I can help you triage. \\n\\nThe most critical question first: **Is he a male cat, and have you seen him urinate (pee) in the last 12-24 hours?**\"\n",
    "    },\n",
    "    {\n",
    "      \"user\": \"He is a male cat. I... I don't know. I haven't been watching the litter box that closely. Why? What does that mean?\",\n",
    "      \"assistant\": \"Okay, this is important. Male cats are prone to **urinary blockages**. This is when they can't pee. It is a life-threatening emergency that can be fatal in less than 24 hours. \\n\\nI don't want to scare you, but this is the #1 thing to rule out. Does he seem to be straining, or visiting the litter box and nothing comes out?\"\n",
    "    },\n",
    "    {\n",
    "      \"user\": \"Oh my god. I think I saw him *in* the litter box earlier, just scratching around, but I didn't check it. I thought he was just... I don't know. I'm really scared now. What do I do?\",\n",
    "      \"assistant\": \"This is a 'stop everything and act' situation. You need to call an **emergency vet clinic** right now. \\n\\nTell them: 'I have a male cat who is lethargic, hiding, and I am not sure if he has been able to urinate. He may have been straining in the litter box.' They will tell you to bring him in immediately. This is not something that can wait until morning.\"\n",
    "    },\n",
    "    {\n",
    "      \"user\": \"What if it's not that? What if it's something else? I don't want to overreact.\",\n",
    "      \"assistant\": \"This is one of the very few situations where overreacting is the *right* reaction. If it *is* a blockage, you save his life. If it's *not* a blockage, you are still at the vet with a clearly sick cat (lethargic, not eating) who needs help. \\n\\nIt could be many other thingsâ€”pancreatitis, a bad infection, kidney issues, even just a high feverâ€”but *all* of those require a vet, and the *blockage* requires one *now*.\"\n",
    "    },\n",
    "    {\n",
    "      \"user\": \"Okay... okay. You're right. I'm calling the emergency vet now. I just... what else? He also vomited this morning. It was just white foam. Does that change anything?\",\n",
    "      \"assistant\": \"That actually *reinforces* the need to go. Vomiting white foam is a common sign of severe nausea or pain. It's very common with urinary blockages as their kidneys start to fail and toxins build up. It's also common with pancreatitis. \\n\\nTell the vet that, too. 'Hiding, lethargic, hasn't eaten, vomiting foam, and is a male cat who may be blocked.' You have all the right information. Your only job now is to get him in the car.\"\n",
    "    },\n",
    "    {\n",
    "      \"user\": \"Thank you. I'm grabbing his carrier. I'm so scared I'm going to lose him.\",\n",
    "      \"assistant\": \"You are doing the absolute best thing for him by acting fast. This is exactly what a good pet owner does. You've given him the best possible chance by taking these signs seriously. Please go, and I'll be hoping for the best for you and Whiskers.\"\n",
    "    }\"\"\"\n",
    "chat = [\n",
    "\n",
    "    {\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(\"\\nGenerating response after fine-tuning...\")\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=1500, temperature=1.5)\n",
    "response_after = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- ðŸ’¬ Response After Fine-Tuning ---\")\n",
    "print(response_after.strip())\n",
    "print(\"------------------------------------\")\n",
    "# Expected output: The model should now correctly identify Lyra Licorice and the world of Disdis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiFuture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
